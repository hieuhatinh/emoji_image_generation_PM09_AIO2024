{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Import các thư viện cần thiết"
      ],
      "metadata": {
        "id": "k5MzVOCwyr2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from diffusers import AutoencoderKL\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import CLIPTokenizer, CLIPTextModel\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.amp import GradScaler, autocast"
      ],
      "metadata": {
        "id": "WWBMW9NAyrsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cố định tham số ngẫu nhiên"
      ],
      "metadata": {
        "id": "e5IA2Mjoyyhj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuNTu43Cypgz"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    print(f\"Seed set to {seed}\")\n",
        "\n",
        "set_seed()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tải bộ dữ liệu"
      ],
      "metadata": {
        "id": "Bo2C-3RGy7j1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://drive.google.com/file/d/15Z_F4Dwgb3NLqEGnVMUEJqyxXgW7Gx-h/view?usp=sharing\n",
        "!gdown 15Z_F4Dwgb3NLqEGnVMUEJqyxXgW7Gx-h\n",
        "!unzip blobs_crawled_data.zip"
      ],
      "metadata": {
        "id": "ThbiMt_my6N8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Xây dựng các class Attention"
      ],
      "metadata": {
        "id": "8ThxZFZ1zGKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, num_attn_heads, hidden_dim, in_proj_bias=True, out_proj_bias=True):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_attn_heads\n",
        "        self.head_size = hidden_dim // num_attn_heads\n",
        "        self.qkv_proj = nn.Linear(hidden_dim, 3 * hidden_dim, bias=in_proj_bias)\n",
        "        self.output_proj = nn.Linear(hidden_dim, hidden_dim, bias=out_proj_bias)\n",
        "\n",
        "    def forward(self, features, use_causal_mask=False):\n",
        "        b, s, d = features.shape\n",
        "        qkv_combined = self.qkv_proj(features)\n",
        "        q_mat, k_mat, v_mat = torch.chunk(qkv_combined, 3, dim=-1)\n",
        "        q_mat = q_mat.view(b, s, self.num_heads, self.head_size).permute(0, 2, 1, 3)\n",
        "        k_mat = k_mat.view(b, s, self.num_heads, self.head_size).permute(0, 2, 1, 3)\n",
        "        v_mat = v_mat.view(b, s, self.num_heads, self.head_size).permute(0, 2, 1, 3)\n",
        "\n",
        "        qk = torch.matmul(q_mat, k_mat.transpose(-2,-1))\n",
        "        sqrt_qk = qk / math.sqrt(self.head_size)\n",
        "\n",
        "        if use_causal_mask:\n",
        "            causal_mask = torch.triu(torch.ones_like(sqrt_qk, dtype=torch.bool), diagonal=1)\n",
        "            sqrt_qk = sqrt_qk.masked_fill(causal_mask,-torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(sqrt_qk, dim=-1)\n",
        "        attn_values = torch.matmul(attn_weights, v_mat)\n",
        "\n",
        "        attn_values = attn_values.permute(0, 2, 1, 3).contiguous()\n",
        "        attn_values = attn_values.view(b, s, d)\n",
        "\n",
        "        final_output = self.output_proj(attn_values)\n",
        "        return final_output"
      ],
      "metadata": {
        "id": "1fs6iTZ7y6LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, num_attn_heads, query_dim, context_dim, in_proj_bias=True, out_proj_bias=True):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_attn_heads\n",
        "        self.head_size = query_dim // num_attn_heads\n",
        "\n",
        "        self.query_map = nn.Linear(query_dim, query_dim, bias=in_proj_bias)\n",
        "        self.key_map = nn.Linear(context_dim, query_dim, bias=in_proj_bias)\n",
        "        self.value_map = nn.Linear(context_dim, query_dim, bias=in_proj_bias)\n",
        "\n",
        "        self.output_map = nn.Linear(query_dim, query_dim, bias=out_proj_bias)\n",
        "\n",
        "    def forward(self, query_input, context_input):\n",
        "        b_q, s_q, d_q = query_input.shape\n",
        "        _, s_kv, _ = context_input.shape\n",
        "\n",
        "        q_mat = self.query_map(query_input)\n",
        "        k_mat = self.key_map(context_input)\n",
        "        v_mat = self.value_map(context_input)\n",
        "\n",
        "        q_mat = q_mat.view(b_q, s_q, self.num_heads, self.head_size).permute(0, 2, 1, 3)\n",
        "        k_mat = k_mat.view(b_q, s_kv, self.num_heads, self.head_size).permute(0, 2, 1, 3)\n",
        "        v_mat = v_mat.view(b_q, s_kv, self.num_heads, self.head_size).permute(0, 2, 1, 3)\n",
        "\n",
        "        qk = torch.matmul(q_mat, k_mat.transpose(-2,-1))\n",
        "        sqrt_qk = qk / math.sqrt(self.head_size)\n",
        "        attn_weights = torch.softmax(sqrt_qk, dim=-1)\n",
        "\n",
        "        attn_values = torch.matmul(attn_weights, v_mat)\n",
        "        attn_values = attn_values.permute(0, 2, 1, 3).contiguous()\n",
        "        attn_values = attn_values.view(b_q, s_q, d_q)\n",
        "\n",
        "        final_output = self.output_map(attn_values)\n",
        "        return final_output"
      ],
      "metadata": {
        "id": "2IdAGpqWy6Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Khai báo class DDPM"
      ],
      "metadata": {
        "id": "gzxHAdIf0vXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DDPMScheduler:\n",
        "    def __init__(\n",
        "        self,\n",
        "        random_generator,\n",
        "        train_timesteps=1000,\n",
        "        diffusion_beta_start=0.00085,\n",
        "        diffusion_beta_end=0.012\n",
        "    ):\n",
        "    self.betas = torch.linspace(\n",
        "        diffusion_beta_start ** 0.5, diffusion_beta_end ** 0.5, train_timesteps,\n",
        "        dtype=torch.float32) ** 2\n",
        "    self.alphas = 1.0-self.betas\n",
        "    self.alphas_cumulative_product = torch.cumprod(self.alphas, dim=0)\n",
        "    self.one_val = torch.tensor(1.0)\n",
        "    self.prng_generator = random_generator\n",
        "    self.total_train_timesteps = train_timesteps\n",
        "    self.schedule_timesteps = torch.from_numpy(np.arange(0, train_timesteps)[::-1].copy())\n",
        "\n",
        "    def set_steps(self, num_sampling_steps=50):\n",
        "        self.num_sampling_steps = num_sampling_steps\n",
        "        step_scaling_factor = self.total_train_timesteps // self.num_sampling_steps\n",
        "        timesteps_for_sampling = (\n",
        "            np.arange(0, num_sampling_steps) * step_scaling_factor\n",
        "        ).round()[::-1].copy().astype(np.int64)\n",
        "        self.schedule_timesteps = torch.from_numpy(timesteps_for_sampling)\n",
        "\n",
        "    def _get_prior_timestep(self, current_timestep):\n",
        "        previous_t = current_timestep - self.total_train_timesteps//self.num_sampling_steps\n",
        "        return previous_t\n",
        "\n",
        "    def _calculate_variance(self, timestep):\n",
        "        prev_t = self._get_prior_timestep(timestep)\n",
        "        alpha_cumprod_t = self.alphas_cumulative_product[timestep]\n",
        "        alpha_cumprod_t_prev = self.alphas_cumulative_product[prev_t] if prev_t >= 0 else self.one_val\n",
        "        beta_t_current = 1-alpha_cumprod_t / alpha_cumprod_t_prev\n",
        "        variance_value = (1-alpha_cumprod_t_prev) / (1-alpha_cumprod_t) * beta_t_current\n",
        "        variance_value = torch.clamp(variance_value, min=1e-20)\n",
        "        return variance_value\n",
        "\n",
        "    def adjust_strength(self, strength_level=1):\n",
        "        initial_step_index = self.num_sampling_steps-int(self.num_sampling_steps * strength_level)\n",
        "        self.schedule_timesteps = self.schedule_timesteps[initial_step_index:]\n",
        "        self.start_sampling_step = initial_step_index\n",
        "\n",
        "    def step(self, current_t, current_latents, model_prediction):\n",
        "        t = current_t\n",
        "        prev_t = self._get_prior_timestep(t)\n",
        "\n",
        "        alpha_cumprod_t = self.alphas_cumulative_product[t]\n",
        "        alpha_cumprod_t_prev = self.alphas_cumulative_product[prev_t] if prev_t >= 0 else self.one_val\n",
        "        beta_cumprod_t = 1-alpha_cumprod_t\n",
        "        beta_cumprod_t_prev = 1-alpha_cumprod_t_prev\n",
        "        alpha_t_current = alpha_cumprod_t / alpha_cumprod_t_prev\n",
        "        beta_t_current = 1-alpha_t_current\n",
        "\n",
        "        predicted_original = (current_latents-beta_cumprod_t**0.5 * model_prediction) / alpha_cumprod_t**0.5\n",
        "\n",
        "        original_coeff = (alpha_cumprod_t_prev ** 0.5 * beta_t_current) / beta_cumprod_t\n",
        "        current_coeff = alpha_t_current**0.5 * beta_cumprod_t_prev / beta_cumprod_t\n",
        "\n",
        "        predicted_prior_mean = original_coeff*predicted_original + current_coeff*current_latents\n",
        "\n",
        "        variance_term = 0\n",
        "        if t > 0:\n",
        "            target_device = model_prediction.device\n",
        "            noise_component = torch.randn(\n",
        "                model_prediction.shape,\n",
        "                generator=self.prng_generator,\n",
        "                device=target_device,\n",
        "                dtype=model_prediction.dtype\n",
        "            )\n",
        "            variance_term = (self._calculate_variance(t) ** 0.5) * noise_component\n",
        "\n",
        "        predicted_prior_sample = predicted_prior_mean + variance_term\n",
        "        return predicted_prior_sample\n",
        "\n",
        "    def add_noise(self, initial_samples, noise_timesteps):\n",
        "        alphas_cumprod = self.alphas_cumulative_product.to(\n",
        "            device=initial_samples.device,\n",
        "            dtype=initial_samples.dtype\n",
        "        )\n",
        "        noise_timesteps = noise_timesteps.to(initial_samples.device)\n",
        "        sqrt_alpha_cumprod = alphas_cumprod[noise_timesteps] ** 0.5\n",
        "        sqrt_alpha_cumprod = sqrt_alpha_cumprod.view(\n",
        "            sqrt_alpha_cumprod.shape[0], *([1] * (initial_samples.ndim-1))\n",
        "        )\n",
        "        sqrt_one_minus_alpha_cumprod = (1-alphas_cumprod[noise_timesteps]) ** 0.5\n",
        "        sqrt_one_minus_alpha_cumprod = sqrt_one_minus_alpha_cumprod.view(\n",
        "        sqrt_one_minus_alpha_cumprod.shape[0], *([1] * (initial_samples.ndim- 1)))\n",
        "        random_noise = torch.randn(\n",
        "            initial_samples.shape, generator=self.prng_generator,\n",
        "            device=initial_samples.device, dtype=initial_samples.dtype\n",
        "        )\n",
        "        noisy_result = sqrt_alpha_cumprod*initial_samples + sqrt_one_minus_alpha_cumprod*random_noise\n",
        "        return noisy_result, random_noise"
      ],
      "metadata": {
        "id": "NsVlNYJZy6GC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNET_ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, time_dim=1280):\n",
        "        super().__init__()\n",
        "        self.gn_feature = nn.GroupNorm(32, in_channels)\n",
        "        self.conv_feature = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.time_embedding_proj = nn.Linear(time_dim, out_channels)\n",
        "\n",
        "        self.gn_merged = nn.GroupNorm(32, out_channels)\n",
        "        self.conv_merged = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "\n",
        "        if in_channels == out_channels:\n",
        "            self.residual_connection = nn.Identity()\n",
        "        else:\n",
        "            self.residual_connection = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n",
        "\n",
        "    def forward(self, input_feature, time_emb):\n",
        "        residual = input_feature\n",
        "        h = self.gn_feature(input_feature)\n",
        "        h = F.silu(h)\n",
        "        h = self.conv_feature(h)\n",
        "\n",
        "        time_emb_processed = F.silu(time_emb)\n",
        "        time_emb_projected = self.time_embedding_proj(time_emb_processed)\n",
        "        time_emb_projected = time_emb_projected.unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "        merged_feature = h + time_emb_projected\n",
        "        merged_feature = self.gn_merged(merged_feature)\n",
        "        merged_feature = F.silu(merged_feature)\n",
        "        merged_feature = self.conv_merged(merged_feature)\n",
        "\n",
        "        output = merged_feature + self.residual_connection(residual)\n",
        "        return output"
      ],
      "metadata": {
        "id": "pOpANg_xy6DB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNET_AttentionBlock(nn.Module):\n",
        "    def __init__(self, num_heads, head_dim, context_dim=512):\n",
        "        super().__init__()\n",
        "        embed_dim = num_heads * head_dim\n",
        "        self.gn_in = nn.GroupNorm(32, embed_dim, eps=1e-6)\n",
        "        self.proj_in = nn.Conv2d(embed_dim, embed_dim, kernel_size=1, padding=0)\n",
        "        self.ln_1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn_1 = SelfAttention(num_heads, embed_dim, in_proj_bias=False)\n",
        "        self.ln_2 = nn.LayerNorm(embed_dim)\n",
        "        self.attn_2 = CrossAttention(num_heads, embed_dim, context_dim, in_proj_bias=False)\n",
        "        self.ln_3 = nn.LayerNorm(embed_dim)\n",
        "        self.ffn_geglu = nn.Linear(embed_dim, 4 * embed_dim * 2)\n",
        "        self.ffn_out = nn.Linear(4 * embed_dim, embed_dim)\n",
        "        self.proj_out = nn.Conv2d(embed_dim, embed_dim, kernel_size=1, padding=0)\n",
        "\n",
        "    def forward(self, input_tensor, context_tensor):\n",
        "        skip_connection = input_tensor\n",
        "\n",
        "        B, C, H, W = input_tensor.shape\n",
        "        HW = H * W\n",
        "\n",
        "        h = self.gn_in(input_tensor)\n",
        "        h = self.proj_in(h)\n",
        "        h = h.view(B, C, HW).transpose(-1,-2)\n",
        "\n",
        "        attn1_skip = h\n",
        "        h = self.ln_1(h)\n",
        "        h = self.attn_1(h)\n",
        "        h = h + attn1_skip\n",
        "\n",
        "        attn2_skip = h\n",
        "        h = self.ln_2(h)\n",
        "        h = self.attn_2(h, context_tensor)\n",
        "        h = h + attn2_skip\n",
        "\n",
        "        ffn_skip = h\n",
        "        h = self.ln_3(h)\n",
        "        intermediate, gate = self.ffn_geglu(h).chunk(2, dim=-1)\n",
        "        h = intermediate * F.gelu(gate)\n",
        "        h = self.ffn_out(h)\n",
        "        h = h + ffn_skip\n",
        "\n",
        "        h = h.transpose(-1,-2).view(B, C, H, W)\n",
        "        output = self.proj_out(h) + skip_connection\n",
        "        return output"
      ],
      "metadata": {
        "id": "WcOGPLvWy6AS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Upsample(nn.Module):\n",
        "    def __init__(self, num_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, feature_map):\n",
        "        x = F.interpolate(feature_map, scale_factor=2, mode=’nearest’)\n",
        "        x = self.conv(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Ds-wzRlYy59h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SwitchSequential(nn.Sequential):\n",
        "    def forward(self, x, guidance_context, time_embedding):\n",
        "        for module_instance in self:\n",
        "            if isinstance(module_instance, UNET_AttentionBlock):\n",
        "                x = module_instance(x, guidance_context)\n",
        "            elif isinstance(module_instance, UNET_ResidualBlock):\n",
        "                x = module_instance(x, time_embedding)\n",
        "            else:\n",
        "                x = module_instance(x)\n",
        "        return x\n",
        "\n",
        "class TimeEmbedding(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.proj1 = nn.Linear(n_embd, 4 * n_embd)\n",
        "        self.proj2 = nn.Linear(4 * n_embd, 4 * n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj1(x)\n",
        "        x = F.silu(x)\n",
        "        x = self.proj2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "LfpulgAA33gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Xây dựng hàm mã hóa thông tin thời gian"
      ],
      "metadata": {
        "id": "uaqmRAl-4MTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_a_timestep(timestep, embedding_dim=320):\n",
        "    half_dim = embedding_dim // 2\n",
        "    freqs = torch.exp(-math.log(10000) *\n",
        "                      torch.arange(start=0, end=half_dim, dtype=torch.float32)\n",
        "                      / half_dim)\n",
        "    x = torch.tensor([timestep], dtype=torch.float32)[:, None] * freqs[None]\n",
        "    return torch.cat([torch.cos(x), torch.sin(x)], dim=-1)\n",
        "\n",
        "def embed_timesteps(timesteps, embedding_dim=320):\n",
        "    half_dim = embedding_dim // 2\n",
        "    freqs = torch.exp(-math.log(10000) *\n",
        "                      torch.arange(half_dim, dtype=torch.float32) /\n",
        "                      half_dim).to(device=timesteps.device)\n",
        "    args = timesteps[:, None].float() * freqs[None, :]\n",
        "    return torch.cat([torch.cos(args), torch.sin(args)], dim=-1)"
      ],
      "metadata": {
        "id": "ZmQPvwvb4J05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Khai báo Diffusion model"
      ],
      "metadata": {
        "id": "HXEMBJat4eiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Diffusion(nn.Module):\n",
        "    def __init__(self, h_dim=128, n_head=4):\n",
        "        super().__init__()\n",
        "        self.time_embedding = TimeEmbedding(320)\n",
        "        self.unet = UNET(h_dim, n_head)\n",
        "        self.unet_output = UNETOutputLayer(h_dim, 4)\n",
        "\n",
        "    @torch.autocast(\n",
        "        device_type='cuda', dtype=torch.float16,\n",
        "        enabled=True, cache_enabled=True\n",
        "    )\n",
        "    def forward(self, latent, context, time):\n",
        "        time = self.time_embedding(time)\n",
        "        output = self.unet(latent, context, time)\n",
        "        output = self.unet_output(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "Bg6NkMhO33eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Khai báo mô hình CLIP"
      ],
      "metadata": {
        "id": "IuB7VzRm4mVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPTextEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        CLIP_id = \"openai/clip-vit-base-patch32\"\n",
        "        self.tokenizer = CLIPTokenizer.from_pretrained(CLIP_id)\n",
        "        self.text_encoder = CLIPTextModel.from_pretrained(CLIP_id)\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        for param in self.text_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.text_encoder.eval()\n",
        "        self.text_encoder.to(self.device)\n",
        "\n",
        "    def forward(self, prompts):\n",
        "        inputs = self.tokenizer(\n",
        "            prompts,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.text_encoder.config.max_position_embeddings,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        input_ids = inputs.input_ids.to(self.device)\n",
        "        attention_mask = inputs.attention_mask.to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            text_encoder_output = self.text_encoder(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "        last_hidden_states = text_encoder_output.last_hidden_state\n",
        "        return last_hidden_states"
      ],
      "metadata": {
        "id": "G9xbeei033bE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Khai báo mô hình pre-trained VAE"
      ],
      "metadata": {
        "id": "TUs2DfgU42Sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VAE_id = \"stabilityai/sd-vae-ft-mse\"\n",
        "vae = AutoencoderKL.from_pretrained(VAE_id)\n",
        "vae.requires_grad_(False)\n",
        "vae.eval()"
      ],
      "metadata": {
        "id": "Ye3q5mxJ33YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Khai báo hàm trực quan hóa ảnh và scale giá trị ảnh"
      ],
      "metadata": {
        "id": "HbPDeLA446yS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_images(images, title=\"\", titles=[]):\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    for i in range(min(25, len(images))):\n",
        "        plt.subplot(5, 5, i+1)\n",
        "        img = images[i].permute(1, 2, 0).cpu().numpy()\n",
        "        plt.imshow(img)\n",
        "        if titles:\n",
        "            plt.title(titles[i])\n",
        "        plt.axis(\"off\")\n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def rescale(value, in_range, out_range, clamp=False):\n",
        "    in_min, in_max = in_range\n",
        "    out_min, out_max = out_range\n",
        "\n",
        "    in_span = in_max-in_min\n",
        "    out_span = out_max-out_min\n",
        "\n",
        "    scaled_value = (value-in_min) / (in_span + 1e-8)\n",
        "    rescaled_value = out_min + (scaled_value * out_span)\n",
        "\n",
        "    if clamp:\n",
        "        rescaled_value = torch.clamp(\n",
        "            rescaled_value,\n",
        "            out_min, out_max\n",
        "        )\n",
        "\n",
        "    return rescaled_value"
      ],
      "metadata": {
        "id": "vHKHMipc44tf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Khai báo class PyTorch dataset"
      ],
      "metadata": {
        "id": "eiS6DUvQ5NVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WIDTH, HEIGHT = 32, 32\n",
        "batch_size = 32\n",
        "\n",
        "class EmojiDataset(Dataset):\n",
        "    def __init__(self, csv_files, image_folder, transform=None):\n",
        "        self.dataframe = pd.concat([pd.read_csv(csv_file) for csv_file in csv_files])\n",
        "        self.images_folder = image_folder\n",
        "        self.dataframe[\"image_path\"] = self.dataframe[\"file_name\"].str.replace(\"\\\\\", \"/\")\n",
        "        self.image_paths = self.dataframe[\"image_path\"].tolist()\n",
        "        self.titles = self.dataframe[\"prompt\"].tolist()\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.images_folder + \"/\" + self.image_paths[idx]\n",
        "        title = self.titles[idx]\n",
        "        title = title.replace('\"', \"\").replace(\"'\", \"\")\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, title"
      ],
      "metadata": {
        "id": "pnZt8n2244qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(\n",
        "        (WIDTH, HEIGHT),\n",
        "        interpolation=transforms.InterpolationMode.BICUBIC\n",
        "    ),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "csv_files = ['/content/blobs_crawled_data/metadata.csv']\n",
        "image_folder = '/content/blobs_crawled_data/images'\n",
        "\n",
        "train_dataset = EmojiDataset(\n",
        "    csv_files=csv_files,\n",
        "    image_folder=image_folder,\n",
        "    transform=transform\n",
        ")\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True\n",
        ")"
      ],
      "metadata": {
        "id": "kbB8tkFR44oN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Thực hiện huấn luyện"
      ],
      "metadata": {
        "id": "ogYxNL275zbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(diffusion, vae, text_encoder, scheduler,\n",
        "          optimizer, lr_scheduler, scaler,\n",
        "          criterion, dataloader, num_epochs, device=\"cuda\"):\n",
        "    losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        diffusion.train()\n",
        "\n",
        "        epoch_loss = 0.0\n",
        "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\",\n",
        "                            leave=False)\n",
        "\n",
        "        for batch_idx, (images, titles) in enumerate(progress_bar):\n",
        "            images = images.to(device)\n",
        "            image_titles = [f\"A photo of {title}\" for title in titles]\n",
        "            image_titles = [title if random.random() < 0.5 else \"\" for title in image_titles]\n",
        "            with torch.no_grad():\n",
        "                latents = vae.encode(images).latent_dist.sample() * 0.18215\n",
        "\n",
        "            timesteps = torch.randint(\n",
        "                0, scheduler.total_train_timesteps,\n",
        "                 (latents.shape[0],), device=device\n",
        "                )\n",
        "\n",
        "            noisy_latents, noise = scheduler.add_noise(latents, timesteps)\n",
        "            time_embeddings = embed_timesteps(timesteps).to(device)\n",
        "            text_embeddings = text_encoder(image_titles)\n",
        "\n",
        "            noise_pred = diffusion(noisy_latents, text_embeddings, time_embeddings)\n",
        "\n",
        "            with autocast(device_type=\"cuda\", dtype=torch.float16,\n",
        "                          enabled=True, cache_enabled=True):\n",
        "                loss = criterion(noise_pred, noise)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            batch_loss = loss.item()\n",
        "            epoch_loss += batch_loss\n",
        "\n",
        "            progress_bar.set_postfix(loss=f\"{batch_loss:.5f}\",\n",
        "                                     lr=f\"{optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "        lr_scheduler.step()\n",
        "        avg_epoch_loss = epoch_loss / len(dataloader)\n",
        "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}]-Avg Loss: {avg_epoch_loss:.5f}\")\n",
        "\n",
        "        losses.append(avg_epoch_loss)\n",
        "\n",
        "    print(\"Training finished!\")\n",
        "    return losses"
      ],
      "metadata": {
        "id": "xCItRkgF44k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 300\n",
        "\n",
        "h_dim = 384\n",
        "n_head = 8\n",
        "\n",
        "vae = vae.to(device)\n",
        "diffusion = Diffusion(h_dim, n_head).to(device)\n",
        "clip = CLIPTextEncoder().to(device)\n",
        "\n",
        "random_generator = torch.Generator(device=\"cuda\")\n",
        "noise_scheduler = DDPMScheduler(random_generator)\n",
        "\n",
        "optimizer = torch.optim.AdamW(diffusion.parameters(), lr=1e-4)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "lrate_scheduler = lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer, T_max=EPOCHS, eta_min=1e-5\n",
        ")\n",
        "scaler = GradScaler()\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "vae_params = count_parameters(vae)\n",
        "diffusion_params = count_parameters(diffusion)\n",
        "clip_params = count_parameters(clip)\n",
        "\n",
        "print(f\"VAE parameters: {vae_params}\")\n",
        "print(f\"Diffusion parameters: {diffusion_params}\")\n",
        "print(f\"CLIP parameters: {clip_params}\")\n",
        "print(f\"Total parameters: {vae_params + diffusion_params + clip_params}\")\n",
        "\n",
        "losses = train(diffusion, vae, clip, noise_scheduler,\n",
        "               optimizer, lrate_scheduler, scaler,\n",
        "               criterion, train_dataloader, EPOCHS, device=device)"
      ],
      "metadata": {
        "id": "4b3B0yun51BN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(diffusion.state_dict(), \"Emoji_SD.pth\")\n",
        "!zip-r Emoji_SD.zip Emoji_SD.pth"
      ],
      "metadata": {
        "id": "to25I6Lk50-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Triển khai mô hình"
      ],
      "metadata": {
        "id": "cBZtYPvS61b9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Xây dựng hàm sinh ảnh"
      ],
      "metadata": {
        "id": "qk9CEli864NE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LATENTS_WIDTH = WIDTH // 8\n",
        "LATENTS_HEIGHT = HEIGHT // 8\n",
        "\n",
        "def generate_image(\n",
        "    prompt,\n",
        "    diffusion,\n",
        "    vae,\n",
        "    text_encoder,\n",
        "    scheduler,\n",
        "    num_inference_steps=100,\n",
        "    seed=None,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "):\n",
        "    rng_generator = torch.Generator(device=device)\n",
        "    if seed is None:\n",
        "        rng_generator.seed()\n",
        "    else:\n",
        "        rng_generator.manual_seed(seed)\n",
        "\n",
        "    prompts = [prompt]\n",
        "    text_embeddings = text_encoder(prompts).to(device)\n",
        "\n",
        "    scheduler.set_steps(num_inference_steps)\n",
        "\n",
        "    latent_shape = (1, 4, LATENTS_HEIGHT, LATENTS_WIDTH)\n",
        "\n",
        "    noisy_latents = torch.randn(\n",
        "        latent_shape, generator=rng_generator, device=device)\n",
        "    timesteps = scheduler.schedule_timesteps\n",
        "\n",
        "    for t in tqdm(timesteps):\n",
        "        latent_model_input = noisy_latents\n",
        "        time_embedding = embed_a_timestep(t).to(device)\n",
        "        with torch.no_grad():\n",
        "            noise_pred = diffusion(\n",
        "                latent_model_input,\n",
        "                text_embeddings,\n",
        "                time_embedding\n",
        "            )\n",
        "        noisy_latents = scheduler.step(\n",
        "            t,\n",
        "            noisy_latents,\n",
        "            noise_pred\n",
        "        )\n",
        "\n",
        "    final_latents = noisy_latents / 0.18215\n",
        "    with torch.no_grad():\n",
        "        decoded_image_tensor = vae.decode(final_latents).sample\n",
        "    image_output = rescale(decoded_image_tensor, (-1, 1), (0, 255), clamp=True)\n",
        "    image_output = image_output.permute(0, 2, 3, 1).to(\"cpu\", torch.uint8).numpy()\n",
        " return image_output[0]"
      ],
      "metadata": {
        "id": "2Becu1FW50qv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}